from cmath import log
import time
from typing import cast, Dict, Any
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langgraph.graph import StateGraph, END
from langgraph.errors import GraphInterrupt
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI

from agent.core.state import SimplerAgendaState
from agent.core.agent import AgentCore
from agent.core.router import RouterLogic
from agent.core.session import SessionManager
from agent.utils.formatters import ResponseFormatter

load_dotenv()

class AgendaAgent:
    """TATAæ•…äº‹åˆ›ä½œåŠ©æ‰‹çš„ä¸»è¦å…¥å£ç±»"""
    
    def __init__(self, verbose=True, user_name: str = "user_main", log_level="INFO", 
                 database_manager=None):
        self.user_name = user_name
        self.database_manager = database_manager
        
        # åˆå§‹åŒ–LLM
        llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
        
        # ğŸ¯ ä¼ é€’æ•°æ®åº“ç®¡ç†å™¨ç»™ AgentCore
        self.agent_core = AgentCore(
            user_name=user_name,
            verbose=verbose,
            log_level=log_level,
            database_manager=database_manager  # ğŸ¯ æ–°å¢ä¼ é€’
        )
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.router_logic = RouterLogic(self.agent_core.logger)
        self.session_manager = SessionManager(database_manager, user_name, self.agent_core.logger)
        self.formatter = ResponseFormatter()
        
        # å¿«é€Ÿè®¿é—®å±æ€§
        self.user_name = user_name
        self.database_manager = database_manager
        self.logger = self.agent_core.logger
        self.stream_callback = None
        
        # æ„å»ºå·¥ä½œæµå›¾
        self.workflow = StateGraph(SimplerAgendaState)
        self._setup_graph()
        self.graph = self.workflow.compile()

    def set_stream_callback(self, callback):
        """è®¾ç½®æµå¼å›è°ƒå‡½æ•°"""
        self.stream_callback = callback
        self.agent_core.set_stream_callback(callback)

    def _send_stream_event(self, event_type: str, content: str, metadata: dict = None):
        """å‘é€æµå¼äº‹ä»¶çš„è¾…åŠ©æ–¹æ³•"""
        if self.stream_callback:
            try:
                return self.stream_callback(event_type, content, metadata or {})
            except Exception as e:
                self.logger.error(f"æµå¼å›è°ƒé”™è¯¯: {e}")
                return None
        return None

    def _setup_graph(self):
        """æ„å»ºå·¥ä½œæµå›¾"""
        # æ·»åŠ èŠ‚ç‚¹ - ä½¿ç”¨åŒ…è£…å™¨æ–¹æ³•æ¥æ”¯æŒæµå¼
        self.workflow.add_node("initializer", self._initializer_node_wrapper)
        self.workflow.add_node("planner", self._planner_node_wrapper) 
        self.workflow.add_node("router", self._router_node_wrapper)
        self.workflow.add_node("tool", self._tool_node_wrapper)
        
        # è®¾ç½®å…¥å£ç‚¹å’Œè¾¹
        self.workflow.set_entry_point("initializer")
        self.workflow.add_edge("initializer", "planner")
        self.workflow.add_edge("planner", "router")
        
        # è·¯ç”±é€»è¾‘ - ä½¿ç”¨åŒ…è£…å™¨ç¡®ä¿çŠ¶æ€æ­£ç¡®ä¼ é€’
        self.workflow.add_conditional_edges(
            "router",
            self._router_condition_wrapper,
            {
                "call_tool": "tool",
                "ask_human": END,
                "continue_planning": "planner",
                "finish": END
            }
        )
        self.workflow.add_edge("tool", "planner")

    def _initializer_node_wrapper(self, state: SimplerAgendaState) -> SimplerAgendaState:
        """åˆå§‹åŒ–èŠ‚ç‚¹åŒ…è£…å™¨"""
        self._send_stream_event('thinking', 'æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡...', {'step_name': 'ä»»åŠ¡åˆå§‹åŒ–'})
        self.logger.info("[***åç«¯ç³»ç»Ÿ] æ­£åœ¨åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€...")
        return self.agent_core.node_manager.initializer_node(state)

    def _planner_node_wrapper(self, state: SimplerAgendaState) -> SimplerAgendaState:
        """è§„åˆ’èŠ‚ç‚¹åŒ…è£…å™¨ - å¢å¼ºæµå¼æ”¯æŒ"""
        self._send_stream_event('thinking', 'æ­£åœ¨è§„åˆ’ä¸‹ä¸€æ­¥...', {'step_name': 'ç­–ç•¥è§„åˆ’'})
        
        # è°ƒç”¨åŸå§‹è§„åˆ’èŠ‚ç‚¹
        result = self.agent_core.node_manager.planner_node(state)
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šå¦‚æœplanner_nodeç”Ÿæˆäº†LLMå“åº”å†…å®¹ï¼Œç¼“å­˜åˆ°SessionManager
        llm_response_content = result.get("_llm_response_content")
        if llm_response_content:
            self.session_manager.cache_llm_response(llm_response_content)
            self.logger.info(f"ğŸ¯ å·²å°†LLMå“åº”å†…å®¹ç¼“å­˜åˆ°SessionManager")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„è‰ç¨¿å†…å®¹
        if result.get('draft_outputs'):
            for draft_id, content in result.get('draft_outputs', {}).items():
                # å‘é€å®Œæ•´çš„è‰ç¨¿å†…å®¹
                full_content = str(content)
                self.logger.info(f"å‘é€è‰ç¨¿æ›´æ–°: {draft_id} ({len(full_content)} å­—ç¬¦)")
                self._send_stream_event('draft_update', f'ç”Ÿæˆè‰ç¨¿: {draft_id}', {
                    'draft_id': draft_id,
                    'content': full_content,  # å®Œæ•´å†…å®¹
                    'updated_by': 'ai'
                })
        
        return result

    def _router_node_wrapper(self, state: SimplerAgendaState) -> SimplerAgendaState:
        """è·¯ç”±èŠ‚ç‚¹åŒ…è£…å™¨"""
        self._send_stream_event('thinking', 'æ­£åœ¨å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨...', {'step_name': 'å†³ç­–è·¯ç”±'})
        return state  # è·¯ç”±é€»è¾‘ç°åœ¨åœ¨æ¡ä»¶è¾¹ä¸­å¤„ç†

    def _router_condition_wrapper(self, state: SimplerAgendaState) -> str:
        """è·¯ç”±æ¡ä»¶åŒ…è£…å™¨ - ç¡®ä¿çŠ¶æ€ä¿®æ”¹èƒ½å¤Ÿæ­£ç¡®ä¼ é€’"""
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šåœ¨æ¡ä»¶è¾¹ä¸­ä¿ç•™LLMå“åº”å†…å®¹
        llm_response_content = state.get("_llm_response_content")
        
        # è°ƒç”¨åŸå§‹è·¯ç”±é€»è¾‘
        decision = self.router_logic.should_call_tool(state)
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šç¡®ä¿LLMå“åº”å†…å®¹åœ¨è·¯ç”±å†³ç­–åä»ç„¶å­˜åœ¨
        if llm_response_content and not state.get("_llm_response_content"):
            state["_llm_response_content"] = llm_response_content
            self.logger.info(f"ğŸ”§ åœ¨è·¯ç”±æ¡ä»¶ä¸­æ¢å¤LLMå“åº”å†…å®¹ ({len(llm_response_content)} å­—ç¬¦)")
        
        # ğŸ¯ è°ƒè¯•ï¼šè®°å½•è·¯ç”±å†³ç­–åçš„çŠ¶æ€
        final_llm_content = state.get("_llm_response_content")
        self.logger.info(f"ğŸ” è·¯ç”±æ¡ä»¶æ‰§è¡Œå:")
        self.logger.info(f"  - å†³ç­–ç»“æœ: {decision}")
        self.logger.info(f"  - LLMå“åº”å†…å®¹å­˜åœ¨: {bool(final_llm_content)}")
        if final_llm_content:
            self.logger.info(f"  - LLMå“åº”å†…å®¹é•¿åº¦: {len(final_llm_content)} å­—ç¬¦")
        
        return decision

    def _tool_node_wrapper(self, state: SimplerAgendaState) -> SimplerAgendaState:
        """å·¥å…·èŠ‚ç‚¹åŒ…è£…å™¨ - å¢å¼ºæµå¼æ”¯æŒ"""
        tool_name = state.get("tool_name")
        tool_params = state.get("tool_params", {})
        tool_call_id = state.get("tool_call_id_for_next_tool_message")
        
        if tool_name and self.stream_callback:
            # ğŸ¯ è·å–å·¥å…·æ˜¾ç¤ºåç§°ï¼ŒåŒ…æ‹¬æ–°çš„æ—…æ¸¸å·¥å…·
            tool_display_name = self.agent_core.get_tool_display_name(tool_name)
            
            # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
            self._send_stream_event('tool_call', f'æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_display_name}', {
                'call_id': tool_call_id or f'tool_{int(time.time())}',
                'tool_name': tool_name,
                'tool_display_name': tool_display_name,
                'params': tool_params
            })
        
        # è°ƒç”¨åŸå§‹å·¥å…·èŠ‚ç‚¹
        result = self.agent_core.node_manager.tool_node(state)
        
        # ğŸ¯ ç‰¹æ®Šå¤„ç†æ—…æ¸¸å·¥å…·çš„ç»“æœ
        if tool_name and tool_name.startswith('travel_') and self.stream_callback:
            # å‘é€æ—…æ¸¸è§„åˆ’ç‰¹å®šäº‹ä»¶
            messages = result.get('messages', [])
            if messages:
                last_message = messages[-1]
                if hasattr(last_message, 'content'):
                    tool_result = str(last_message.content)
                    
                    # ğŸ¯ å¯¹æ—…æ¸¸å·¥å…·ç»“æœè¿›è¡Œç‰¹æ®Šå¤„ç†
                    if tool_name == 'travel_info_extractor':
                        self._send_stream_event('travel_analysis', 'æ•°æ®åˆ†æå®Œæˆ', {
                            'call_id': tool_call_id or f'tool_{int(time.time())}',
                            'analysis_result': tool_result[:500],
                            'full_result': tool_result
                        })
                    elif tool_name in ['travel_planner', 'itinerary_planner']:
                        self._send_stream_event('travel_plan', 'è¡Œç¨‹è§„åˆ’å®Œæˆ', {
                            'call_id': tool_call_id or f'tool_{int(time.time())}',
                            'plan_preview': tool_result[:500],
                            'full_plan': tool_result
                        })
                    else:
                        self._send_stream_event('tool_result', tool_result[:500], {
                            'call_id': tool_call_id or f'tool_{int(time.time())}',
                            'tool_name': tool_name,
                            'result': tool_result
                        })
        elif tool_name and self.stream_callback:
            # æ™®é€šå·¥å…·çš„ç»“æœå¤„ç†
            messages = result.get('messages', [])
            if messages:
                last_message = messages[-1]
                if hasattr(last_message, 'content'):
                    tool_result = str(last_message.content)
                    self._send_stream_event('tool_result', tool_result[:500], {
                        'call_id': tool_call_id or f'tool_{int(time.time())}',
                        'tool_name': tool_name,
                        'result': tool_result
                    })
        
        return result

    def run_interactive_streaming(self, initial_query: str, session_id: str = None, max_iterations: int = 15):
        """æµå¼ç‰ˆæœ¬çš„äº¤äº’å¼è¿è¡Œ"""
        self.logger.info(f"å¼€å§‹æµå¼äº¤äº’è¿è¡Œ: {initial_query}")
        
        # å‘é€åˆå§‹åˆ†æäº‹ä»¶
        yield self._send_stream_event('thinking', 'å¼€å§‹åˆ†æä»»åŠ¡...', {'step_name': 'ä»»åŠ¡åˆ†æ'})
        
        # ğŸ¯ ä¿®å¤ï¼šä¼ é€’ session_id å‚æ•°
        result = self.run_interactive(initial_query, session_id, max_iterations)
        
        # ç¼“å­˜ç»“æœä¾›åç»­è·å–
        self.final_result_cache = result
        
        # åªåœ¨çœŸæ­£å®Œæˆæ—¶å‘é€finaläº‹ä»¶
        if not result.get('is_interactive_pause'):
            yield self._send_stream_event('final', 'å¤„ç†å®Œæˆ', {'step_name': 'å®Œæˆ'})

    def get_final_result(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆç»“æœ - ç¡®ä¿åŒ…å«è®®ç¨‹ä¿¡æ¯"""
        if not hasattr(self, 'current_state') or not self.current_state:
            self.logger.warning("ğŸ“‹ [ä¿®å¤] current_state ä¸å­˜åœ¨æˆ–ä¸ºç©º")
            return {}
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ä¸ run_interactive ç›¸åŒçš„æ ¼å¼åŒ–å™¨
        result = self.formatter.format_final_response(cast(SimplerAgendaState, self.current_state))
        
        self.logger.info(f"ğŸ“‹ [ä¿®å¤] æœ€ç»ˆç»“æœ {result}")
        self.logger.info(f"ğŸ“‹ [ä¿®å¤] æœ€ç»ˆå›ç­”: {result}")
        agenda_found = result.get('final_agenda') or result.get('agenda_doc') or result.get('updated_agenda_doc')
        self.logger.info(f"ğŸ“‹ [ä¿®å¤] è®®ç¨‹ä¿¡æ¯å­˜åœ¨: {bool(agenda_found)}")
        
        return result
    
    def run_interactive(self, initial_query: str, session_id: str, max_iterations: int = 15):
        """è¿è¡Œäº¤äº’å¼å¯¹è¯"""
        
        # ä½¿ç”¨ SessionManager åˆå§‹åŒ–ä¼šè¯
        session_id, current_state_dict, existing_session = self.session_manager.initialize_session(
            initial_query, session_id
        )
        
        # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºé…ç½®åˆå§‹åŒ–è¯·æ±‚
        if self._is_config_initialization(initial_query):
            result = self._handle_config_initialization(initial_query, session_id, current_state_dict)
            return result
        
        # æ‰§è¡Œä¸»å¾ªç¯
        config: RunnableConfig = {"recursion_limit": max_iterations * 3}
        
        for i in range(max_iterations):
            self.logger.info(f"\n--- Iteration {i + 1}/{max_iterations} (Session: {session_id}) ---")
            
            # åœ¨æ¯æ¬¡è¿­ä»£å¼€å§‹æ—¶ï¼Œæ‰“å°å½“å‰æ¶ˆæ¯å†å²æ•°é‡
            messages = current_state_dict.get('messages', [])
            self.logger.info(f"ğŸ“‹ å½“å‰æ¶ˆæ¯å†å²: {len(messages)} æ¡")
            
            # æ£€æŸ¥äº¤äº’æš‚åœçŠ¶æ€
            if current_state_dict.get("is_interactive_pause"):
                self.logger.info("æ£€æµ‹åˆ°äº¤äº’æš‚åœçŠ¶æ€ï¼Œç»“æŸå¤„ç†")
                break
            
            try:
                # ğŸ¯ å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç† invoke çš„è¿”å›å€¼
                # å½“å›¾æ­£å¸¸ç»“æŸæ—¶ï¼ˆå¦‚ action: finishï¼‰ï¼Œinvokeçš„è¿”å›æ˜¯æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„çŠ¶æ€
                # æˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæœ€ç»ˆçŠ¶æ€ä¸å½“å‰çŠ¶æ€åˆå¹¶ï¼Œä»¥ç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½ä¿ç•™
                output_state_dict = self.graph.invoke(current_state_dict, config=config)

                # å¦‚æœ invoke è¿”å›äº†æœ‰æ•ˆçš„çŠ¶æ€ï¼Œåˆ™æ›´æ–°
                if output_state_dict:
                    # ğŸ¯ å…³é”®ä¿®å¤ï¼šä¿ç•™LLMå“åº”å†…å®¹ï¼Œé˜²æ­¢åœ¨çŠ¶æ€åˆå¹¶æ—¶ä¸¢å¤±
                    llm_response_content = current_state_dict.get("_llm_response_content")
                    
                    # ğŸ¯ è°ƒè¯•ï¼šè®°å½•çŠ¶æ€åˆå¹¶å‰çš„ä¿¡æ¯
                    self.logger.info(f"ğŸ” çŠ¶æ€åˆå¹¶å‰:")
                    self.logger.info(f"  - current_stateæœ‰LLMå“åº”å†…å®¹: {bool(llm_response_content)}")
                    self.logger.info(f"  - output_stateæœ‰LLMå“åº”å†…å®¹: {bool(output_state_dict.get('_llm_response_content'))}")
                    
                    # åˆå¹¶è¿”å›çš„çŠ¶æ€ï¼Œç¡®ä¿ action_needed, final_answer ç­‰æœ€ç»ˆå­—æ®µè¢«æ›´æ–°
                    current_state_dict.update(output_state_dict)
                    
                    # ğŸ¯ å…³é”®ä¿®å¤ï¼šå¦‚æœåŸçŠ¶æ€æœ‰LLMå“åº”å†…å®¹ä½†æ–°çŠ¶æ€æ²¡æœ‰ï¼Œåˆ™æ¢å¤
                    if llm_response_content and not current_state_dict.get("_llm_response_content"):
                        current_state_dict["_llm_response_content"] = llm_response_content
                        self.logger.info(f"ğŸ”§ æ¢å¤LLMå“åº”å†…å®¹åˆ°æœ€ç»ˆçŠ¶æ€ ({len(llm_response_content)} å­—ç¬¦)")
                    
                    # ğŸ¯ è°ƒè¯•ï¼šè®°å½•çŠ¶æ€åˆå¹¶åçš„ä¿¡æ¯
                    final_llm_content = current_state_dict.get("_llm_response_content")
                    self.logger.info(f"ğŸ” çŠ¶æ€åˆå¹¶å:")
                    self.logger.info(f"  - æœ€ç»ˆçŠ¶æ€æœ‰LLMå“åº”å†…å®¹: {bool(final_llm_content)}")
                    if final_llm_content:
                        self.logger.info(f"  - LLMå“åº”å†…å®¹é•¿åº¦: {len(final_llm_content)} å­—ç¬¦")
                else:
                    # ç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼Œä½†ä½œä¸ºé˜²æŠ¤
                    self.logger.warning("Graph execution returned None, which is unexpected.")

                # ä¿å­˜çŠ¶æ€åˆ°æ•°æ®åº“
                self.session_manager.save_session_state(session_id, current_state_dict)
                
                # æ£€æŸ¥ç»“æŸæ¡ä»¶
                if self.formatter.should_end_iteration(current_state_dict, i, self.logger):
                    self.logger.info("âœ… æ£€æµ‹åˆ°ç»“æŸæ¡ä»¶ï¼Œè·³å‡ºå¾ªç¯")
                    break
                    
            except GraphInterrupt as gi:
                self.logger.info(f"âœ… GraphInterruptæ•è·ï¼Œæ­£å¸¸ç»“æŸ: {gi}")
                current_state_dict["is_interactive_pause"] = True
                if not current_state_dict.get("final_answer") and current_state_dict.get("human_question"):
                    current_state_dict["final_answer"] = current_state_dict.get("human_question")
                break
                
            except Exception as e:
                self.logger.error(f"æ‰§è¡Œé”™è¯¯: {e}")
                current_state_dict["error_message"] = f"æ‰§è¡Œé”™è¯¯: {str(e)}"
                current_state_dict["final_answer"] = "å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯ã€‚"
                break
        
        # ä¿å­˜AIå“åº”
        self.session_manager.save_ai_response(session_id, current_state_dict, initial_query)
        
        # ğŸ¯ å…³é”®ä¿®å¤ï¼šä¿å­˜å½“å‰çŠ¶æ€ä¾› get_final_result ä½¿ç”¨
        self.current_state = current_state_dict

        result = self.formatter.format_final_response(cast(SimplerAgendaState, current_state_dict))
        result['session_id'] = session_id
        return result
    
    def _build_session_metadata(self, state: SimplerAgendaState) -> Dict[str, Any]:
        """æ„å»ºä¼šè¯å…ƒæ•°æ® - é˜²æ­¢Noneé”™è¯¯"""
        try:
            messages = state.get("messages", [])
            return {
                "message_count": len(messages),
                "tool_calls_made": len(state.get("recent_tool_calls", [])),
                "json_parse_errors": 0,
                "router_errors": 0
            }
        except Exception as e:
            self.logger.error(f"æ„å»ºä¼šè¯å…ƒæ•°æ®å¤±è´¥: {e}")
            return {
                "message_count": 0,
                "tool_calls_made": 0,
                "json_parse_errors": 0,
                "router_errors": 0
            }
    
    def _is_config_initialization(self, query: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé…ç½®åˆå§‹åŒ–è¯·æ±‚"""
        try:
            import json
            data = json.loads(query)
            return isinstance(data, dict) and 'user_profile' in data
        except (json.JSONDecodeError, TypeError):
            return False
    
    def _handle_config_initialization(self, query: str, session_id: str, state: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†é…ç½®åˆå§‹åŒ–è¯·æ±‚ - ä¿®å¤æ ¹ä»»åŠ¡è®¾ç½®å’ŒJSONå­˜å‚¨"""
        try:
            import json
            config_data = json.loads(query)
            
            user_profile = config_data.get('user_profile')
            travel_query = config_data.get('travel_query', '')  # ğŸ¯ æ­£ç¡®æå– travel_query
            
            self.logger.info(f"ğŸ¯ é…ç½®åˆå§‹åŒ–æ•°æ®: user_profile={user_profile}, travel_query={travel_query[:50]}...")
            
            # ğŸ¯ ç”Ÿæˆå¼€åœºç™½ï¼ˆç°åœ¨è¿”å›æ–‡æœ¬ï¼Œä½†å†…éƒ¨ç¼“å­˜äº†JSONï¼‰
            opening_message = self._generate_opening_message(user_profile, travel_query)
            
            # ğŸ¯ å…³é”®æ–°å¢ï¼šè·å–ç¼“å­˜çš„JSONå“åº”å¹¶ä¿å­˜åˆ°çŠ¶æ€
            if hasattr(self, '_cached_opening_response') and self._cached_opening_response:
                cached_response = self._cached_opening_response
                if cached_response['json_response']:
                    # ä¿å­˜å®Œæ•´çš„JSONå“åº”åˆ°çŠ¶æ€ï¼Œä¾›æ•°æ®åº“å­˜å‚¨ä½¿ç”¨
                    state["_llm_response_content"] = json.dumps(
                        cached_response['json_response'],
                        ensure_ascii=False,
                        indent=2
                    )
                    self.logger.info(f"âœ… å¼€åœºç™½JSONå“åº”å·²ä¿å­˜åˆ°çŠ¶æ€")
                else:
                    # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä¿å­˜åŸå§‹å“åº”
                    state["_llm_response_content"] = cached_response['raw_response']
                    self.logger.info(f"âœ… å¼€åœºç™½åŸå§‹å“åº”å·²ä¿å­˜åˆ°çŠ¶æ€")
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šè®¾ç½®æ­£ç¡®çš„æ ¹ä»»åŠ¡
            root_task = travel_query  # ä½¿ç”¨å®Œæ•´çš„æ—…æ¸¸æŸ¥è¯¢ä½œä¸ºæ ¹ä»»åŠ¡
            initial_agenda = f"- [ ] {root_task} @overall_goal"
            
            # æ›´æ–°çŠ¶æ€
            state.update({
                'input_query': root_task,  # ğŸ¯ å…³é”®ï¼šè®¾ç½®æ­£ç¡®çš„æ ¹ä»»åŠ¡
                'user_profile': user_profile,
                'travel_query': travel_query,
                'final_answer': opening_message,
                'is_interactive_pause': True,  # ğŸ¯ å…³é”®ï¼šè®¾ç½®ä¸ºæš‚åœçŠ¶æ€ï¼Œè§¦å‘ai_pauseç±»å‹
                'action_needed': 'ask_human',  # ğŸ¯ è®¾ç½®ä¸ºè¯¢é—®ç”¨æˆ·
                'human_question': opening_message,  # ğŸ¯ è®¾ç½®human_question
                'agenda_doc': f"# å·¥ä½œè®®ç¨‹\n\n{initial_agenda}\n\n**ç”¨æˆ·æ¡£æ¡ˆ**: {user_profile}\n**ä»»åŠ¡è¯¦æƒ…**: {travel_query[:100]}{'...' if len(travel_query) > 100 else ''}"
            })
            
            # ä¿å­˜çŠ¶æ€
            self.session_manager.save_session_state(session_id, state)
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨è°ƒç”¨save_ai_responseæ¥å­˜å‚¨å¼€åœºç™½
            self.session_manager.save_ai_response(session_id, state, query)
            self.logger.info(f"ğŸ¯ å¼€åœºç™½å·²ä½œä¸ºai_pauseç±»å‹å­˜å‚¨åˆ°æ•°æ®åº“")
            
            # æ ¼å¼åŒ–å“åº”
            result = self.formatter.format_final_response(cast(SimplerAgendaState, state))
            result['session_id'] = session_id
            
            self.logger.info(f"ğŸ¯ é…ç½®åˆå§‹åŒ–å®Œæˆï¼Œå¼€åœºç™½å°†ä½œä¸ºai_pauseç±»å‹å­˜å‚¨")
            
            return result
            
        except Exception as e:
            self.logger.error(f"é…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
            return {
                'final_answer': 'é…ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·é‡è¯•',
                'is_interactive_pause': False,
                'error_message': str(e),
                'session_id': session_id
            }
    
    def _generate_opening_message(self, user_profile: str, destination: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆä¸ªæ€§åŒ–å¼€åœºç™½ - æ”¯æŒJSONæ ¼å¼å’Œæ•°æ®åº“å­˜å‚¨"""
        try:
            # ğŸ¯ ä¿®å¤ï¼šç¡®ä¿æ­£ç¡®å¯¼å…¥LLM
            from langchain_core.messages import SystemMessage, HumanMessage
            from langchain_openai import ChatOpenAI
            import os
            import json
            
            # ğŸ¯ å…³é”®ä¿®å¤ï¼šä»æ•°æ®åº“è·å–çœŸå®çš„ç”¨æˆ·æ¡£æ¡ˆä¿¡æ¯
            user_capabilities = {'description': 'é€šç”¨ç”¨æˆ·', 'display_name': 'é»˜è®¤ç”¨æˆ·'}
            try:
                if self.database_manager:
                    # ğŸ¯ æ–°å¢ï¼šæ£€æŸ¥æ•°æ®åº“ç®¡ç†å™¨çš„ç±»å‹å’Œå±æ€§
                    self.logger.info(f"æ•°æ®åº“ç®¡ç†å™¨ç±»å‹: {type(self.database_manager)}")
                    self.logger.info(f"æ•°æ®åº“ç®¡ç†å™¨å±æ€§: {dir(self.database_manager)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ get_user_profile æ–¹æ³•
                    if hasattr(self.database_manager, 'get_user_profile'):
                        # ç›´æ¥ä»æ•°æ®åº“è·å–ç”¨æˆ·æ¡£æ¡ˆ
                        profile_data = self.database_manager.get_user_profile(user_profile)
                        if profile_data:
                            user_capabilities = {
                                'description': profile_data.get('overall_profile', 'é€šç”¨ç”¨æˆ·æ¡£æ¡ˆ'),
                                'display_name': profile_data.get('display_name', profile_data.get('name', 'ç”¨æˆ·')),
                                'experiment_group': profile_data.get('experiment_group', ''),
                                'user_type': profile_data.get('user_type', 'general')
                            }
                            self.logger.info(f"ğŸ¯ æˆåŠŸè·å–ç”¨æˆ·æ¡£æ¡ˆ: {user_capabilities['display_name']}")
                        else:
                            self.logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ç”¨æˆ·æ¡£æ¡ˆ: {user_profile}")
                    else:
                        self.logger.error(f"æ•°æ®åº“ç®¡ç†å™¨ç¼ºå°‘ get_user_profile æ–¹æ³•")
                else:
                    self.logger.warning("æ•°æ®åº“ç®¡ç†å™¨ä¸å¯ç”¨")
            except Exception as e:
                self.logger.error(f"è·å–ç”¨æˆ·æ¡£æ¡ˆå¤±è´¥: {e}")
                import traceback
                self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                # ğŸ¯ ä¿æŒåŸæœ‰çš„å›é€€é€»è¾‘ä½œä¸ºæœ€åçš„å¤‡é€‰
                if user_profile == 'travel_expert':
                    user_capabilities = {'description': 'ä¸“ä¸šçš„æ—…æ¸¸è§„åˆ’ä¸“å®¶', 'display_name': 'æ—…æ¸¸ä¸“å®¶'}
                elif user_profile == 'budget_traveler':
                    user_capabilities = {'description': 'æ³¨é‡æ€§ä»·æ¯”çš„æ—…è¡Œè€…', 'display_name': 'ç»æµæ—…è¡Œè€…'}
                elif user_profile == 'luxury_traveler':
                    user_capabilities = {'description': 'è¿½æ±‚é«˜ç«¯ä½“éªŒçš„æ—…è¡Œè€…', 'display_name': 'è±ªåæ—…è¡Œè€…'}
                else:
                    user_capabilities = {'description': 'é€šç”¨ç”¨æˆ·æ¡£æ¡ˆï¼Œé€‚åº”æ€§å¼º', 'display_name': 'é€šç”¨ç”¨æˆ·'}
            
            # ğŸ¯ ä¿®æ”¹ï¼šæ„å»ºè¦æ±‚JSONè¾“å‡ºçš„ç³»ç»Ÿæç¤º
            system_prompt = """ä½ å«TATAï¼Œ|ä¸€ä¸ªäººæœºåä½œå®Œæˆä»»åŠ¡çš„ä¸“ä¸šçš„AIåä½œè€…ï¼ˆæ­¤éƒ¨åˆ†æ— éœ€å‘ŠçŸ¥äººç±»ï¼‰|ã€‚è¯·ç”Ÿæˆä¸€ä¸ªä¸ªæ€§åŒ–çš„ã€æ˜“è¯»çš„å¼€åœºç™½ã€‚

1. å‘ŠçŸ¥ç”¨æˆ·ä½ çš„åå­—ï¼Œå¹¶ä¸”æ¥ä¸‹æ¥ä½ ä¼šå’Œç”¨æˆ·ä¸€èµ·å®Œæˆé‡è¦çš„ä»»åŠ¡ï¼Œç›´åˆ°ç”¨æˆ·å®Œæˆå®éªŒé—®å·çš„å¡«å†™ã€‚
2. å‘ŠçŸ¥ç”¨æˆ·åç»­äº¤äº’è§„åˆ™ï¼šä½œä¸ºAIï¼Œä½ ä¼šååŠ©è¿™æ¬¡ä»»åŠ¡çš„è¿›è¡Œï¼Œä½ æŒæ¡ç›¸å…³çš„ä¿¡æ¯ã€è§„åˆ’èƒ½åŠ›ã€è®¡ç®—èƒ½åŠ›ã€‚ä½†æ˜¯ä½ ä¹Ÿéœ€è¦äººç±»çš„å¸®åŠ©æ¥å®Œæˆä¸€äº›ä»»åŠ¡ï¼Œæ¯”å¦‚æä¾›ä¿¡æ¯è¡¥å……ã€å®¡æ ¸å®¡é˜…ç­‰ã€‚è¯·äººç±»å¸®åŠ©ä½ å®Œæˆåç»­çš„ä»»åŠ¡ã€‚
3. å‘ŠçŸ¥æ¥ä¸‹æ¥ä½ å’Œç”¨æˆ·ä¼šä¸€èµ·å®Œæˆä»€ä¹ˆä»»åŠ¡ã€‚
4. å‘ŠçŸ¥ç”¨æˆ·ï¼Œtaéœ€è¦å®Œæˆçš„ä»»åŠ¡ï¼š
    ç†æ€§çš„åˆ¤æ–­ï¼š
    *å› ä¸ºä½ ä½œä¸ºAIä¹Ÿå¯èƒ½çŠ¯é”™*ï¼Œæ¯”å¦‚è®¡ç®—é”™è¯¯ã€è§„åˆ’é”™è¯¯ç­‰ï¼ˆéœ€è¦å‘ŠçŸ¥ç”¨æˆ·ï¼‰ã€‚æ‰€ä»¥ä¸ºäº†æäº¤ä»»åŠ¡æ­£ç¡®ç‡æœ€é«˜ï¼Œéœ€è¦è¯·äººç±»å¸®åŠ©ä½ å®¡é˜…å’Œç¡®è®¤ä½ çš„å·¥ä½œã€‚å…·ä½“æ¥è¯´ï¼š
   - ç¡®è®¤é¢„ç®—è®¡ç®—å‡†ç¡®æ— è¯¯
   - ä¿è¯è¡Œç¨‹å®‰æ’é—­ç¯ï¼Œä»å‡ºå‘åœ°åˆ°ç›®çš„åœ°å†å›åˆ°å‡ºå‘åœ°
   - æ ¸å¯¹äº¤é€šæ–¹å¼
   - æ ¸å¯¹æ™¯ç‚¹é€‰æ‹©ï¼Œç¬¦åˆéœ€æ±‚
   - ç¡®è®¤é¤é¥®å®‰æ’ï¼Œç¬¦åˆå£å‘³
   - ç¡®è®¤ä½å®¿å®‰æ’ç¬¦åˆçº¦æŸï¼Œæ¯”å¦‚æœ€å°å…¥ä½å¤©æ•°ã€æœ€å¤§äººæ•°ã€‚
   åå¥½çš„æŒ‡å®šï¼š
   - æœ‰å“ªäº›å–œå¥½çš„æ™¯ç‚¹\é¤é¥®\ä½å®¿\äº¤é€šç­‰ã€‚
5. å¼•å¯¼ç”¨æˆ·"å¦‚æœå‡†å¤‡å¥½äº†ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹äº†~â˜ºï¸
ä¸­æ–‡è¾“å‡ºã€‚

**é‡è¦ï¼šä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼š**
{
    "human_question": "ä½ çš„å®Œæ•´å¼€åœºç™½å†…å®¹ï¼Œè¦å®Œå…¨ç¬¦åˆéœ€æ±‚ï¼Œ*å¿…é¡»æ¢è¡Œã€åˆ†ç‚¹ã€åˆ—ç‚¹è¾“å‡º*ã€‚",
    "why_need_human": "Cognitive judgment|Creativity|External world interaction|Domain expertise knowledge|Private domain information|Preference constraints|Responsibility scope|User-authorizable contentï¼ˆå¤šé€‰ï¼Œéœ€å¤šæ ·æ€§å¤§ï¼‰",
    "when_need_human": "Decision-making needs|Innovation needs|Execution needs|Professional knowledge needs|Private information needs|Personal preference needs|Responsibility assumption needs|User authorization needsï¼ˆå¤šé€‰ï¼Œéœ€å¤šæ ·æ€§å¤§ï¼‰",
    "interaction_behavior": "Prime|Configure|Guide|Explainï¼ˆå…¨é€‰ï¼‰",
    "communication_principle": "Echoing responses|Casual language|Feedback|Using emoji|Emphatic messages|Humor|Present capabilities|Acknowledge limitations|Repetitive messages|Exaggerationï¼ˆå¤šé€‰ï¼Œéœ€å¤šæ ·æ€§å¤§ï¼‰"
}"""

            # æ„å»ºç”¨æˆ·æç¤º - ä½¿ç”¨ä¸ªæ€§åŒ–æ¡£æ¡ˆä¿¡æ¯
            user_prompt = f"""ç”¨æˆ·æ¡£æ¡ˆï¼š{user_capabilities.get('display_name', user_profile)}
æ¡£æ¡ˆæè¿°ï¼š{user_capabilities.get('description')}
æ¥ä¸‹æ¥ï¼Œä½ ä»¬ä¼šä¸€èµ·å®Œæˆçš„ä»»åŠ¡è¦æ±‚ï¼š{destination}ï¼›æ³¨æ„è¿™ä¸ªä»»åŠ¡*ä¸æ˜¯ç”¨æˆ·çš„éœ€æ±‚*ï¼Œè€Œæ˜¯æˆ‘ä»¬ä¸€èµ·è§„åˆ’ä¸€ä¸ªä»»åŠ¡ã€‚

è¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“æ¡£æ¡ˆç‰¹å¾ç”Ÿæˆä¸€ä¸ªä¸ªæ€§åŒ–çš„å¼€åœºç™½ï¼Œä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºã€‚"""

            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # è°ƒç”¨LLM
            self.logger.info(f"prompt: {user_prompt}")
            model_name = os.getenv("OPENAI_MODEL", "gpt-4o")
            llm = ChatOpenAI(model=model_name, temperature=0.7)
            response = llm.invoke(messages)
            
            # ğŸ¯ æ–°å¢ï¼šè§£æJSONå“åº”å¹¶ç¼“å­˜
            try:
                response_content = response.content.strip()
                
                # å¤„ç†å¯èƒ½çš„```jsonåŒ…è£…
                if '```json' in response_content:
                    start = response_content.find('```json') + 7
                    end = response_content.find('```', start)
                    if end > start:
                        json_content = response_content[start:end].strip()
                    else:
                        json_content = response_content
                else:
                    json_content = response_content
                
                parsed_response = json.loads(json_content)
                
                # ğŸ¯ å…³é”®ï¼šç¼“å­˜å®Œæ•´çš„JSONå“åº”ä¾›åç»­ä½¿ç”¨
                self._cached_opening_response = {
                    'json_response': parsed_response,
                    'raw_response': response_content
                }
                
                # è¿”å›human_questionä½œä¸ºå¼€åœºç™½æ–‡æœ¬
                opening_text = parsed_response.get('human_question', response_content)
                self.logger.info(f"âœ… AIå¼€åœºç™½JSONè§£ææˆåŠŸ")
                return opening_text
                
            except json.JSONDecodeError as e:
                self.logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å“åº”: {e}")
                # å›é€€å¤„ç†
                self._cached_opening_response = {
                    'json_response': None,
                    'raw_response': response.content
                }
                return response.content
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¼€åœºç™½å¤±è´¥: {e}")
            # ğŸ¯ æä¾›ç»“æ„åŒ–çš„é»˜è®¤å›é€€
            default_response = {
                "human_question": f"æ‚¨å¥½ï¼æˆ‘æ˜¯TATAï¼Œæ‚¨çš„ä¸“ä¸šåŠ©æ‰‹ã€‚æˆ‘æ³¨æ„åˆ°æ‚¨å¯¹ **{destination}** å¾ˆæ„Ÿå…´è¶£ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„å…·ä½“éœ€æ±‚ï¼Œæˆ‘ä¼šä¸ºæ‚¨æä¾›ä¸“ä¸šçš„å»ºè®®å’Œæ”¯æŒã€‚",
                "why_need_human": "Cognitive judgment|Domain expertise knowledge|Private domain information|Preference constraints",
                "when_need_human": "Decision-making needs|Professional knowledge needs|Private information needs|Personal preference needs",
                "interaction_behavior": "Prime|Configure|Probe|Guide|Explain",
                "communication_principle": "Casual language|Feedback|Using emoji|Present capabilities|Acknowledge limitations"
            }
            self._cached_opening_response = {
                'json_response': default_response,
                'raw_response': json.dumps(default_response, ensure_ascii=False)
            }
            return default_response['human_question']

